{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad4312",
   "metadata": {
    "id": "YEcc-D52gu4E"
   },
   "source": [
    "# on complete data set baseline model without weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3be8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define transforms to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout with probability 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.conv1(x)))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = LeNet5()\n",
    "\n",
    "# Print total parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb637c8b",
   "metadata": {
    "id": "kdkdnJBnv3YO"
   },
   "source": [
    "#Small_mnist data set without weight (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d78d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28, 1))\n",
    "        label = int(self.data.iloc[idx, 0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load the small MNIST dataset from CSV\n",
    "mnist_df = pd.read_csv(\"small_mnist.csv\")\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_df, test_df = train_test_split(mnist_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transforms to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(train_df, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout with probability 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.conv1(x.float())))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = LeNet5()\n",
    "\n",
    "# Print total parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "train_losses = []  # To store train losses for calculating average later\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if epoch > 0:  # Append loss only after epoch 0\n",
    "                train_losses.append(loss.item())\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {}, Loss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "# Calculate average train loss (excluding epoch 0)\n",
    "average_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "print('Average train loss (excluding epoch 0): {:.4f}'.format(average_train_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8a3a8",
   "metadata": {
    "id": "_64RjtZDwChV"
   },
   "source": [
    "#small_mnist data set with weight decay = 0.0005 (baseline model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28, 1))\n",
    "        label = int(self.data.iloc[idx, 0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load the small MNIST dataset from CSV\n",
    "mnist_df = pd.read_csv(\"small_mnist.csv\")\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_df, test_df = train_test_split(mnist_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transforms to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(train_df, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "# Define LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout with probability 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.conv1(x.float())))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = LeNet5()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "train_losses = []  # To store train losses for calculating average later\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if epoch > 0:  # Append loss only after epoch 0\n",
    "                train_losses.append(loss.item())\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {}, Loss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "# Calculate average train loss (excluding epoch 0)\n",
    "average_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "print('Average train loss (excluding epoch 0): {:.4f}'.format(average_train_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc939af7",
   "metadata": {
    "id": "5mG5A5o4JBOv"
   },
   "source": [
    "# Implementing Data_grad with all default parameters on MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb445a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define LeNet architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset with 50,000 images for training and 10,000 images for testing\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainset, _ = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with default parameters\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Evaluate on test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %.2f %%' % (100 * correct / total))\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f149fcd",
   "metadata": {
    "id": "5CVqFsNRPFR-"
   },
   "source": [
    "#with optimal data_grad weight 50 + without weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28))\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load Your Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomMNISTDataset(csv_file='small_mnist.csv', transform=transform)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define LeNet architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))  # Cast input to float to match bias data type\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bbbb2",
   "metadata": {
    "id": "Hqu31ig0hx96"
   },
   "source": [
    "#Data grad   50 + weight decay = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Define LeNet architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))  # Cast input to float to match bias data type\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "target_grad = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Get the gradient norm and adjust learning rate\n",
    "        grad_norm = nn.utils.clip_grad_norm_(net.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382faebb",
   "metadata": {
    "id": "7KRutYCbjESd"
   },
   "source": [
    "#Spect Reg + with weight decay on small_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44188d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28))\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load your CSV dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomMNISTDataset(csv_file='small_mnist.csv', transform=transform)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define LeNet architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))  # Cast input to float to match bias data type\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "# Target gradient for spectral regularization\n",
    "target_grad = 50\n",
    "\n",
    "# Apply spectral normalization to the linear layers\n",
    "net.fc1 = nn.utils.spectral_norm(net.fc1)\n",
    "net.fc2 = nn.utils.spectral_norm(net.fc2)\n",
    "net.fc3 = nn.utils.spectral_norm(net.fc3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Spectral regularization\n",
    "        spectreg_loss = 0\n",
    "        for name, module in net.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Compute the spectral regularization loss\n",
    "                spectreg_loss += torch.norm(module.weight_u, p=2)\n",
    "\n",
    "        # Add the spectral regularization loss to the main loss spectral weight = 0.05\n",
    "        loss += 0.05 * spectreg_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Get the gradient norm and adjust learning rate\n",
    "        grad_norm = nn.utils.clip_grad_norm_(net.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e18bf",
   "metadata": {
    "id": "GfCLoI1MjONx"
   },
   "source": [
    "# spectreg without weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83291963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28))\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load your CSV dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomMNISTDataset(csv_file='small_mnist.csv', transform=transform)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define LeNet architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))  # Cast input to float to match bias data type\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "# Target gradient for spectral regularization\n",
    "target_grad = 50\n",
    "\n",
    "# Apply spectral normalization to the linear layers\n",
    "net.fc1 = nn.utils.spectral_norm(net.fc1)\n",
    "net.fc2 = nn.utils.spectral_norm(net.fc2)\n",
    "net.fc3 = nn.utils.spectral_norm(net.fc3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Spectral regularization\n",
    "        spectreg_loss = 0\n",
    "        for name, module in net.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Compute the spectral regularization loss\n",
    "                spectreg_loss += torch.norm(module.weight_u, p=2)\n",
    "\n",
    "        # Add the spectral regularization loss to the main loss with spectral weight = 0.03\n",
    "        loss += 0.03 * spectreg_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Get the gradient norm and adjust learning rate\n",
    "        grad_norm = nn.utils.clip_grad_norm_(net.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4d085",
   "metadata": {
    "id": "iqXHb_ScjW9w"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb41fa33",
   "metadata": {
    "id": "cZeAijjbjfBo"
   },
   "source": [
    "#CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44827760",
   "metadata": {
    "id": "mXFmSjuTrUho"
   },
   "source": [
    "# base line model without weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ciphar 10 base line model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and testing datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48392e84",
   "metadata": {
    "id": "Lvx5kV81rl-O"
   },
   "source": [
    "#bse line model with weight decay = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # Transformations for the dataset\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "# ])\n",
    "\n",
    "# # Load CIFAR-10 training and testing datasets\n",
    "# trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay= 0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc03e0",
   "metadata": {
    "id": "rUzPIrG6r8U0"
   },
   "source": [
    "# spect reg = 0.001 without weight decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and testing datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Spectral regularization weight\n",
    "spectral_weight = 0.001\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute spectral regularization loss\n",
    "        spectreg_loss = 0\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                weight = module.weight.view(module.weight.size(0), -1)\n",
    "                u, _, _ = torch.svd(weight)\n",
    "                spectreg_loss += torch.norm(u, p=2)\n",
    "\n",
    "        # Add spectral regularization loss to the main loss\n",
    "        loss += spectreg_loss * spectral_weight\n",
    "\n",
    "        # Step with optimizer\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48ed83",
   "metadata": {
    "id": "WIBwydgbtCwb"
   },
   "source": [
    "# spect reg = 0.03 with weight decay = 0.0005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and testing datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Spectral regularization weight\n",
    "spectral_weight = 0.03\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute spectral regularization loss\n",
    "        spectreg_loss = 0\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                weight = module.weight.view(module.weight.size(0), -1)\n",
    "                u, _, _ = torch.svd(weight)\n",
    "                spectreg_loss += torch.norm(u, p=2)\n",
    "\n",
    "        # Add spectral regularization loss to the main loss\n",
    "        loss += spectreg_loss * spectral_weight\n",
    "\n",
    "        # Step with optimizer\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477331a1",
   "metadata": {
    "id": "ZH_WbL_yutIa"
   },
   "source": [
    "# ciphar-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3ecee",
   "metadata": {
    "id": "efTts1uxu1ub"
   },
   "source": [
    "# base line + without weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09534d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100  # Adjusted for CIFAR-100 which has 100 classes\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 20 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "\n",
    "# Choose only 1000 samples for testing\n",
    "test_indices = range(1000)\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44afbc",
   "metadata": {
    "id": "jHXspXa4u7O-"
   },
   "source": [
    "#base line + with weight decay = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da988113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100  # Adjusted for CIFAR-100 which has 100 classes\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 20 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "\n",
    "# Choose only 1000 samples for testing\n",
    "test_indices = range(1000)\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a275c7",
   "metadata": {
    "id": "oZsRsZtYvE_D"
   },
   "source": [
    "#spectreg = 0.001 + without weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100  # Adjusted for CIFAR-100 which has 100 classes\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 20 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "\n",
    "# Choose only 1000 samples for testing\n",
    "test_indices = range(1000)\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Function to calculate spectral norm of weights\n",
    "def calculate_spectral_norm(model):\n",
    "    spectral_norm = 0\n",
    "    for param in model.parameters():\n",
    "        if param.dim() > 1:  # Exclude biases\n",
    "            spectral_norm += torch.norm(param, p='fro')\n",
    "    return spectral_norm\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Add spectral regularization\n",
    "        spectral_weight = 0.001\n",
    "        spectral_norm = calculate_spectral_norm(model)\n",
    "        loss += spectral_weight * spectral_norm\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516dd73d",
   "metadata": {
    "id": "y-e1EgPBvN83"
   },
   "source": [
    "#spectreg = 0.0003 + weight decay = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eced10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100  # Adjusted for CIFAR-100 which has 100 classes\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 20 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "\n",
    "# Choose only 1000 samples for testing\n",
    "test_indices = range(1000)\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Function to calculate spectral norm of weights\n",
    "def calculate_spectral_norm(model):\n",
    "    spectral_norm = 0\n",
    "    for param in model.parameters():\n",
    "        if param.dim() > 1:  # Exclude biases\n",
    "            spectral_norm += torch.norm(param, p='fro')\n",
    "    return spectral_norm\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Add spectral regularization\n",
    "        spectral_weight = 0.0003\n",
    "        spectral_norm = calculate_spectral_norm(model)\n",
    "        loss += spectral_weight * spectral_norm\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ea6cb",
   "metadata": {
    "id": "WV-lTlnaCkuY"
   },
   "source": [
    "# Small_MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51959bb6",
   "metadata": {
    "id": "ChA2KSqiQ3JN"
   },
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28, 1))\n",
    "        label = int(self.data.iloc[idx, 0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load the small MNIST dataset from CSV\n",
    "mnist_df = pd.read_csv(\"small_mnist.csv\")\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_df, test_df = train_test_split(mnist_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transforms to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(train_df, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout with probability 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.conv1(x.float())))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = LeNet5()\n",
    "\n",
    "# Print total parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "train_losses = []  # To store train losses for calculating average later\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if epoch > 0:  # Append loss only after epoch 0\n",
    "                train_losses.append(loss.item())\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {}, Loss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "# Calculate average train loss (excluding epoch 0)\n",
    "average_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "print('Average train loss (excluding epoch 0): {:.4f}'.format(average_train_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8bbd90",
   "metadata": {
    "id": "bwWii_qwRKDt"
   },
   "source": [
    "#Double Back with optimal weight = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28))\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load Your Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomMNISTDataset(csv_file='small_mnist.csv', transform=transform)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Define LeNet architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))  # Cast input to float to match bias data type\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "target_grad = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Get the gradient norm and adjust learning rate\n",
    "        grad_norm = nn.utils.clip_grad_norm_(net.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbe38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55b7aaa9",
   "metadata": {
    "id": "gKy3bpPNRVxT"
   },
   "source": [
    "#SpectReg with optimal weight = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28))\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load your CSV dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomMNISTDataset(csv_file='small_mnist.csv', transform=transform)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define LeNet architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))  # Cast input to float to match bias data type\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "# Target gradient for spectral regularization\n",
    "target_grad = 50\n",
    "\n",
    "# Apply spectral normalization to the linear layers\n",
    "net.fc1 = nn.utils.spectral_norm(net.fc1)\n",
    "net.fc2 = nn.utils.spectral_norm(net.fc2)\n",
    "net.fc3 = nn.utils.spectral_norm(net.fc3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Spectral regularization\n",
    "        spectreg_loss = 0\n",
    "        for name, module in net.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Compute the spectral regularization loss\n",
    "                spectreg_loss += torch.norm(module.weight_u, p=2)\n",
    "\n",
    "        # Add the spectral regularization loss to the main loss spectral optimal weight = 0.03\n",
    "        loss += 0.03 * spectreg_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Get the gradient norm and adjust learning rate\n",
    "        grad_norm = nn.utils.clip_grad_norm_(net.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31906fc",
   "metadata": {
    "id": "1LBW6gzrRac9"
   },
   "source": [
    "#CP with optimal weight = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04bc1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28, 1))\n",
    "        label = int(self.data.iloc[idx, 0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load the small MNIST dataset from CSV\n",
    "mnist_df = pd.read_csv(\"small_mnist.csv\")\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_df, test_df = train_test_split(mnist_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transforms to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(train_df, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout with probability 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.conv1(x.float())))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = LeNet5()\n",
    "\n",
    "# Print total parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Confidence Penalty (CP) regularization coefficient\n",
    "cp_coefficient = 0.01  # You can adjust this value as needed\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "train_losses = []  # To store train losses for calculating average later\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Confidence Penalty (CP) regularization\n",
    "        confidence_penalty = -torch.mean(torch.sum(torch.softmax(output, dim=1) * torch.log_softmax(output, dim=1), dim=1))\n",
    "        loss += cp_coefficient * confidence_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if epoch > 0:  # Append loss only after epoch 0\n",
    "                train_losses.append(loss.item())\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {}, Loss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "# Calculate average train loss (excluding epoch 0)\n",
    "average_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "print('Average train loss (excluding epoch 0): {:.4f}'.format(average_train_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d9cd4",
   "metadata": {
    "id": "icK1uf4SRhqX"
   },
   "source": [
    "#CP= 0.01 + DoubleBack = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28))\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load Your Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomMNISTDataset(csv_file='small_mnist.csv', transform=transform)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define LeNet architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))  # Cast input to float to match bias data type\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "# Define the parameters for double back and confidence penalty regularization\n",
    "double_back_weight = 10\n",
    "cp_weight = 0.01\n",
    "\n",
    "# Target gradient for double back\n",
    "target_grad = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate gradient norm for double back regularization\n",
    "        grad_norm = nn.utils.clip_grad_norm_(net.parameters(), max_norm=target_grad)\n",
    "\n",
    "        # Backward pass with double back regularization\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Apply double back regularization\n",
    "        for param in net.parameters():\n",
    "            param.grad *= double_back_weight / (grad_norm + 1e-8)\n",
    "\n",
    "        # Confidence Penalty (CP) regularization\n",
    "        confidence_penalty = -torch.mean(torch.sum(torch.softmax(outputs, dim=1) * torch.log_softmax(outputs, dim=1), dim=1))\n",
    "        loss += cp_weight * confidence_penalty\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf0b5e",
   "metadata": {
    "id": "kc_zdZ9uRmz2"
   },
   "source": [
    "#Cp= = 0.01 + Spectral weight = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82089fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data.iloc[idx, 1:].values.astype(float).reshape((28, 28))\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load your CSV dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomMNISTDataset(csv_file='small_mnist.csv', transform=transform)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define LeNet architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))  # Cast input to float to match bias data type\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize LeNet model\n",
    "net = LeNet()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "# Target gradient for spectral regularization\n",
    "target_grad = 50\n",
    "\n",
    "# Apply spectral normalization to the linear layers\n",
    "net.fc1 = nn.utils.spectral_norm(net.fc1)\n",
    "net.fc2 = nn.utils.spectral_norm(net.fc2)\n",
    "net.fc3 = nn.utils.spectral_norm(net.fc3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Confidence Penalty (CP) regularization\n",
    "        confidence_penalty = -torch.mean(torch.sum(torch.softmax(outputs, dim=1) * torch.log_softmax(outputs, dim=1), dim=1))\n",
    "        loss += 0.01 * confidence_penalty\n",
    "\n",
    "        # Spectral regularization\n",
    "        spectreg_loss = 0\n",
    "        for name, module in net.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Compute the spectral regularization loss\n",
    "                spectreg_loss += torch.norm(module.weight_u, p=2)\n",
    "\n",
    "        # Add the spectral regularization loss to the main loss\n",
    "        loss += 0.03 * spectreg_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Get the gradient norm and adjust learning rate\n",
    "        grad_norm = nn.utils.clip_grad_norm_(net.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21e2af",
   "metadata": {
    "id": "8QMbk9aNCkN4"
   },
   "source": [
    "#cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db72b86",
   "metadata": {
    "id": "02bfYgk1RtRp"
   },
   "source": [
    "#Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934cbd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and testing datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef495a",
   "metadata": {
    "id": "2hPSmIEpEEFz"
   },
   "source": [
    "#Double back optimal weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fee584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and testing datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Target weight for double backpropagation\n",
    "target_grad = 1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "        else:\n",
    "          optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bba04",
   "metadata": {
    "id": "cQFtN-k1EZ0d"
   },
   "source": [
    "#spectral = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e570d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and testing datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Target weight for double backpropagation\n",
    "target_grad = 1\n",
    "\n",
    "# Spectral regularization weight\n",
    "spectral_weight = 0.03\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute spectral regularization loss\n",
    "        spectreg_loss = 0\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                weight = module.weight.view(module.weight.size(0), -1)\n",
    "                u, _, _ = torch.svd(weight)\n",
    "                spectreg_loss += torch.norm(u, p=2)\n",
    "\n",
    "        # Add spectral regularization loss to the main loss\n",
    "        loss += spectreg_loss * spectral_weight\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "        else:\n",
    "          optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26e45d",
   "metadata": {
    "id": "ToYd9Hw3OQ5W"
   },
   "source": [
    "#Cp with optimal weight = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631120c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # Transformations for the dataset\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "# ])\n",
    "\n",
    "# # Load CIFAR-10 training and testing datasets\n",
    "# trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function with CP regularization\n",
    "class CPRegularizedLoss(nn.Module):\n",
    "    def __init__(self, base_criterion, confidence_penalty_weight):\n",
    "        super(CPRegularizedLoss, self).__init__()\n",
    "        self.base_criterion = base_criterion\n",
    "        self.confidence_penalty_weight = confidence_penalty_weight\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        base_loss = self.base_criterion(outputs, labels)\n",
    "        softmax_outputs = nn.functional.softmax(outputs, dim=1)\n",
    "        confidence_penalty = -torch.mean(torch.sum(softmax_outputs * torch.log(softmax_outputs + 1e-10), dim=1))\n",
    "        return base_loss + self.confidence_penalty_weight * confidence_penalty\n",
    "\n",
    "criterion = CPRegularizedLoss(nn.CrossEntropyLoss(), confidence_penalty_weight=0.003)\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167da7c",
   "metadata": {
    "id": "7HiZvk85FBN7"
   },
   "source": [
    "# cp + Double back used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and testing datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Confidence Penalty weight\n",
    "cp_weight = 0.003\n",
    "\n",
    "# Target gradient for clipping\n",
    "target_grad = 1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Confidence Penalty term\n",
    "        penalty = cp_weight * torch.mean((torch.softmax(outputs, dim=1) - torch.eye(10)[labels]) ** 2)\n",
    "\n",
    "        # Add penalty to the loss\n",
    "        loss += penalty\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=target_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28839132",
   "metadata": {
    "id": "DWjN6bteGY_E"
   },
   "source": [
    "# cp +spectreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffdec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and testing datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 10\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 200)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Target weight for double backpropagation\n",
    "target_grad = 1\n",
    "\n",
    "# Confidence Penalty weight\n",
    "cp_weight = 0.003\n",
    "\n",
    "# Spectral regularization weight\n",
    "spectral_weight = 0.03\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Compute spectral regularization loss\n",
    "        spectreg_loss = 0\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                weight = module.weight.view(module.weight.size(0), -1)\n",
    "                u, _, _ = torch.svd(weight)\n",
    "                spectreg_loss += torch.norm(u, p=2)\n",
    "\n",
    "        # Add spectral regularization loss to the main loss\n",
    "        loss += spectreg_loss * spectral_weight\n",
    "\n",
    "        # Confidence Penalty term\n",
    "        penalty = cp_weight * torch.mean((torch.softmax(outputs, dim=1) - torch.eye(10)[labels]) ** 2)\n",
    "\n",
    "\n",
    "        # Add penalty to the loss\n",
    "        loss += penalty\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a0d24",
   "metadata": {
    "id": "sF4cw28eVpNR"
   },
   "source": [
    "#CIPHER-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db46648",
   "metadata": {
    "id": "TqyJJq6FkBVe"
   },
   "source": [
    "#base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6dccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100  # Adjusted for CIFAR-100 which has 100 classes\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 20 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "\n",
    "# Choose only 1000 samples for testing\n",
    "test_indices = range(1000)\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.003)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e07d35",
   "metadata": {
    "id": "WmxKxHgzkdy4"
   },
   "source": [
    "#Doule back without cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 20 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "test_indices = range(1000)  # Take only 1000 samples for testing\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Target weight for double backpropagation\n",
    "target_grad = 0.003\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "        else:\n",
    "          optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd147d79",
   "metadata": {
    "id": "IX19lir7nY_N"
   },
   "source": [
    "#Spectreg without cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 20 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "test_indices = range(1000)  # Take only 1000 samples for testing\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Spectral Regularization weight\n",
    "spectreg_weight = 0.00003\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Spectral Regularization\n",
    "        spectral_loss = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                spectral_loss += torch.norm(param, p='fro')\n",
    "\n",
    "        loss += spectreg_weight * spectral_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6ce99",
   "metadata": {
    "id": "acMsvzCbo_sx"
   },
   "source": [
    "# implementation of CP = 0.00001 + double back =  0.003  = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "test_indices = range(1000)  # Take only 1000 samples for testing\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Target weight for double backpropagation\n",
    "target_grad = 0.003\n",
    "\n",
    "# Confidence Penalty weight\n",
    "cp_weight = 0.00001\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Confidence Penalty\n",
    "        outputs_softmax = nn.functional.softmax(outputs, dim=1)\n",
    "        grad_outputs = torch.autograd.grad(outputs=outputs_softmax, inputs=inputs,\n",
    "                                           grad_outputs=torch.ones_like(outputs_softmax),\n",
    "                                           create_graph=True)[0]\n",
    "        cp_loss = torch.mean(torch.norm(grad_outputs, p=2, dim=1))\n",
    "\n",
    "        loss += cp_weight * cp_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=target_grad)\n",
    "        if grad_norm > target_grad:\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518f707",
   "metadata": {
    "id": "cxb9rBG5pEFR"
   },
   "source": [
    "# cp = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 20 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "test_indices = range(1000)  # Take only 1000 samples for testing\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Confidence Penalty weight\n",
    "cp_weight = 0.00001\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Confidence Penalty\n",
    "        max_prob, _ = torch.max(nn.functional.softmax(outputs, dim=1), dim=1)\n",
    "        cp_loss = torch.mean(max_prob)\n",
    "\n",
    "        loss += cp_weight * cp_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.003)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128fdab8",
   "metadata": {
    "id": "D1DsSP3ZpISX"
   },
   "source": [
    "#Spect Reg = 0.00003 + Cp = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-100 mean and std for each channel\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Function to get indices of limited samples per class\n",
    "def get_limited_indices(dataset, limit_per_class):\n",
    "    indices = []\n",
    "    count_per_class = [0] * 100\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if count_per_class[label] < limit_per_class:\n",
    "            indices.append(i)\n",
    "            count_per_class[label] += 1\n",
    "        if all(count >= limit_per_class for count in count_per_class):\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "# Choose only 200 samples per class for training\n",
    "train_indices = get_limited_indices(trainset, 20)\n",
    "trainset_limited = Subset(trainset, train_indices)\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "trainloader = DataLoader(trainset_limited, batch_size=64, shuffle=True)\n",
    "test_indices = range(1000)  # Take only 1000 samples for testing\n",
    "testset_limited = Subset(testset, test_indices)\n",
    "testloader = DataLoader(testset_limited, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize ResNet-18 model\n",
    "model = ResNet18()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define SGD optimizer with weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
    "\n",
    "# Spectral Regularization weight\n",
    "spectreg_weight = 0.00003\n",
    "\n",
    "# Confidence Penalty weight\n",
    "cp_weight = 0.00001\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(33):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad_(True)  # Set requires_grad=True for inputs\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Spectral Regularization\n",
    "        spectral_loss = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                spectral_loss += torch.norm(param, p='fro')\n",
    "\n",
    "        loss += spectreg_weight * spectral_loss\n",
    "\n",
    "        # Confidence Penalty\n",
    "        max_prob, _ = torch.max(nn.functional.softmax(outputs, dim=1), dim=1)\n",
    "        cp_loss = torch.mean(max_prob)\n",
    "\n",
    "        loss += cp_weight * cp_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch and training loss\n",
    "    print('[Epoch %d] Training Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate on test set after completing training\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on Test Set: %.2f %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfeaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c326d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84536764",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy torch torchvision pytorch-ignite tensorboardX tensorboard opendatasets efficientnet-pytorch\n",
    "\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import models, datasets\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, Precision, Recall\n",
    "from ignite.handlers import LRScheduler, ModelCheckpoint, global_step_from_engine\n",
    "from ignite.contrib.handlers import ProgressBar, TensorboardLogger\n",
    "import ignite.contrib.engines.common as common\n",
    "\n",
    "import opendatasets as od\n",
    "import os\n",
    "from random import randint\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "# Define device to use (CPU or GPU). CUDA = GPU support for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf59ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/akash2sharma/tiny-imagenet\")\n",
    "DATA_DIR = 'tiny-imagenet/tiny-imagenet-200'\n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VALID_DIR = os.path.join(DATA_DIR, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'tiny-imagenet/tiny-imagenet-200'\n",
    "num_classes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738bf6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this depending on memory constraints\n",
    "batch_size = 64\n",
    "\n",
    "# the magic normalization parameters come from the example\n",
    "transform_mean = np.array([ 0.485, 0.456, 0.406 ])\n",
    "transform_std = np.array([ 0.229, 0.224, 0.225 ])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = transform_mean, std = transform_std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = transform_mean, std = transform_std),\n",
    "])\n",
    "\n",
    "traindir = os.path.join(directory, \"train\")\n",
    "# be careful with this set, the labels are not defined using the directory structure\n",
    "valdir = os.path.join(directory, \"val\")\n",
    "\n",
    "train = datasets.ImageFolder(traindir, train_transform)\n",
    "val = datasets.ImageFolder(valdir, val_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "assert num_classes == len(train_loader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_labels = {}\n",
    "with open(os.path.join(directory, \"words.txt\"), \"r\") as dictionary_file:\n",
    "    line = dictionary_file.readline()\n",
    "    while line:\n",
    "        label_id, label = line.strip().split(\"\\t\")\n",
    "        small_labels[label_id] = label\n",
    "        line = dictionary_file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7fe453",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(small_labels.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead69b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(traindir)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf15db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "label_ids = {}\n",
    "for label_index, label_id in enumerate(train_loader.dataset.classes):\n",
    "    label = small_labels[label_id]\n",
    "    labels[label_index] = label\n",
    "    label_ids[label_id] = label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df420227",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(small_labels.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01010d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(label_ids.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d113de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_map = {}\n",
    "with open(os.path.join(directory, \"val/val_annotations.txt\"), \"r\") as val_label_file:\n",
    "    line = val_label_file.readline()\n",
    "    while line:\n",
    "        file_name, label_id, _, _, _, _ = line.strip().split(\"\\t\")\n",
    "        val_label_map[file_name] = label_id\n",
    "        line = val_label_file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(val_label_map.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader.dataset.imgs[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_loader.dataset.imgs)):\n",
    "    file_path = val_loader.dataset.imgs[i][0]\n",
    "\n",
    "    file_name = os.path.basename(file_path)\n",
    "    label_id = val_label_map[file_name]\n",
    "\n",
    "    val_loader.dataset.imgs[i] = (file_path, label_ids[label_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da56883",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader.dataset.imgs[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b9f6d",
   "metadata": {
    "id": "3JVNIGivW7kh"
   },
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    weight_shape = list(m.weight.data.size())\n",
    "    fan_in = weight_shape[1]\n",
    "    fan_out = weight_shape[0]\n",
    "    w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "    m.weight.data.uniform_(-w_bound, w_bound)\n",
    "    m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b44cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = models.alexnet(pretrained=True)\n",
    "state_dict = pre_trained_model.state_dict()\n",
    "state_dict.pop(\"classifier.6.weight\")\n",
    "state_dict.pop(\"classifier.6.bias\")\n",
    "model = models.alexnet(num_classes=num_classes)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "parameters = model.classifier[6].parameters()\n",
    "initialize_weights(model.classifier[6])\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(parameters, lr=0.001, momentum=0.9) #adams optimizer\n",
    "def top_k_error(top_k, total):\n",
    "    return 100.0 - top_k / total * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574eb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train=True, log_every=100):\n",
    "    running_loss = 0.0\n",
    "    running_top_1 = 0.0\n",
    "    running_top_5 = 0.0\n",
    "    running_total = 0.0\n",
    "\n",
    "    epoch_top_1 = 0.0\n",
    "    epoch_top_5 = 0.0\n",
    "    epoch_total = 0.0\n",
    "\n",
    "    model.train(mode=train)\n",
    "\n",
    "    for batch_number, (batch_inputs, batch_labels) in enumerate(loader):\n",
    "        batch_inputs, batch_labels = Variable(batch_inputs), Variable(batch_labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batch_logits = model(batch_inputs)\n",
    "\n",
    "        if train:\n",
    "            batch_loss = criterion(batch_logits, batch_labels)\n",
    "            batch_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += batch_loss.item()\n",
    "\n",
    "\n",
    "        batch_labels = batch_labels.data.cpu().numpy()\n",
    "        batch_predictions = batch_logits.topk(5)[1].data.cpu().numpy()\n",
    "\n",
    "        for i in range(len(batch_labels)):\n",
    "            if batch_labels[i] == batch_predictions[i, 0]:\n",
    "                running_top_1 += 1\n",
    "                running_top_5 += 1\n",
    "                epoch_top_1 += 1\n",
    "                epoch_top_5 += 1\n",
    "            else:\n",
    "                for j in range(1, 5):\n",
    "                    if batch_labels[i] == batch_predictions[i, j]:\n",
    "                        running_top_5 += 1\n",
    "                        epoch_top_5 += 1\n",
    "                        break\n",
    "\n",
    "        running_total += len(batch_labels)\n",
    "        epoch_total += len(batch_labels)\n",
    "\n",
    "        if batch_number % log_every == log_every - 1:\n",
    "            if train:\n",
    "                print(\"[Batch {:5d}] Loss: {:.3f} Top-1 Error: {:.3f} Top-5 Error: {:.3f}\".format(\n",
    "                    batch_number + 1,\n",
    "                    running_loss / log_every,\n",
    "                    top_k_error(running_top_1, running_total),\n",
    "                    top_k_error(running_top_5, running_total)\n",
    "                ))\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_top_1 = 0.0\n",
    "            running_top_5 = 0.0\n",
    "            running_total = 0.0\n",
    "\n",
    "    return top_k_error(epoch_top_1, epoch_total), top_k_error(epoch_top_5, epoch_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1 # modify this to run different number of epochs\n",
    "\n",
    "for epoch_number in range(num_epochs):\n",
    "\n",
    "    train_top_1_error, train_top_5_error = run_epoch(train_loader, train=True)\n",
    "    print(\"[Epoch {:3d}] Training Top-1 Error: {:.3f} Top-5 Error: {:.3f}\".format(\n",
    "        epoch_number + 1, train_top_1_error, train_top_5_error))\n",
    "\n",
    "    val_top_1_error, val_top_5_error = run_epoch(val_loader, train=False)\n",
    "    print(\"[Epoch {:3d}] Validation Top-1 Error: {:.3f} Top-5 Error: {:.3f}\".format(\n",
    "        epoch_number + 1, val_top_1_error, val_top_5_error))\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa49ff01",
   "metadata": {
    "id": "OgHUx3c9WseE"
   },
   "source": [
    "So the accuracy we are getting here in the 8 th batch is :\n",
    "For Top 1 base line is = 24.188 and for Top 5 is = 48.625.\n",
    "due to very less computation power i am stopping at batch 8. accuracy could have been higher if we go with more no. of batches lets say 20 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9809b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(mode=False)\n",
    "\n",
    "num_images = 5 # modify the number of images shown\n",
    "\n",
    "batch_inputs, batch_labels = next(iter(val_loader))\n",
    "batch_inputs = Variable(batch_inputs, volatile=True)\n",
    "\n",
    "batch_logits = model(batch_inputs)\n",
    "\n",
    "batch_labels = batch_labels.numpy()\n",
    "batch_predictions = batch_logits.topk(5)[1].data.cpu().numpy()\n",
    "\n",
    "cell_number = 1\n",
    "\n",
    "plt.figure(figsize=(4, num_images * 2))\n",
    "\n",
    "for image_number in range(num_images):\n",
    "    image = np.copy(batch_inputs.data[image_number].cpu().numpy())\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    for channel in range(3):\n",
    "        image[:, :, channel] = image[:, :, channel] * transform_std[channel] + transform_mean[channel]\n",
    "\n",
    "    label = labels[batch_labels[image_number]]\n",
    "\n",
    "    plt.subplot(num_images, 2, cell_number)\n",
    "\n",
    "    ax = plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    cell_number += 1\n",
    "\n",
    "    plt.subplot(num_images, 2, cell_number)\n",
    "    plt.axis(\"off\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.text(0, 0.85, \"Label: {}\".format(label))\n",
    "    for prediction_number in range(5):\n",
    "        plt.text(0, 0.85 - 0.15 * (prediction_number + 1), \"Prediction-{:d}: {}\".format(\n",
    "            prediction_number + 1, labels[batch_predictions[image_number, prediction_number]]))\n",
    "\n",
    "    cell_number += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb8863",
   "metadata": {
    "id": "fbEuyzDqTepY"
   },
   "source": [
    "# spectral reg with optimal weight  = 20,000 and weight decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e480ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define your model and load pre-trained weights\n",
    "pre_trained_model = models.alexnet(pretrained=True)\n",
    "state_dict = pre_trained_model.state_dict()\n",
    "state_dict.pop(\"classifier.6.weight\")\n",
    "state_dict.pop(\"classifier.6.bias\")\n",
    "model = models.alexnet(num_classes=num_classes)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Initialize the optimizer with the entire model parameters\n",
    "optimizer = AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.0001)\n",
    "\n",
    "# Define your loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def top_k_error(top_k, total):\n",
    "    return 100.0 - top_k / total * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4851cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def run_epoch(loader, model, criterion, optimizer=None, train=True, log_every=100, spectral_weight=20000):\n",
    "    running_loss = 0.0\n",
    "    running_top_1 = 0.0\n",
    "    running_top_5 = 0.0\n",
    "    running_total = 0.0\n",
    "\n",
    "    epoch_top_1 = 0.0\n",
    "    epoch_top_5 = 0.0\n",
    "    epoch_total = 0.0\n",
    "\n",
    "    model.train(mode=train)\n",
    "\n",
    "    for batch_number, (batch_inputs, batch_labels) in enumerate(loader):\n",
    "        batch_inputs, batch_labels = Variable(batch_inputs), Variable(batch_labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batch_logits = model(batch_inputs)\n",
    "\n",
    "        if train:\n",
    "            batch_loss = criterion(batch_logits, batch_labels)\n",
    "\n",
    "            # Spectral Regularization\n",
    "            weight_sum = 0.0\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    weight_sum += torch.sum(param ** 2)\n",
    "            batch_loss += spectral_weight * weight_sum\n",
    "\n",
    "            batch_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += batch_loss.item()\n",
    "\n",
    "\n",
    "        batch_labels = batch_labels.data.cpu().numpy()\n",
    "        batch_predictions = batch_logits.topk(5)[1].data.cpu().numpy()\n",
    "\n",
    "        for i in range(len(batch_labels)):\n",
    "            if batch_labels[i] == batch_predictions[i, 0]:\n",
    "                running_top_1 += 1\n",
    "                running_top_5 += 1\n",
    "                epoch_top_1 += 1\n",
    "                epoch_top_5 += 1\n",
    "            else:\n",
    "                for j in range(1, 5):\n",
    "                    if batch_labels[i] == batch_predictions[i, j]:\n",
    "                        running_top_5 += 1\n",
    "                        epoch_top_5 += 1\n",
    "                        break\n",
    "\n",
    "        running_total += len(batch_labels)\n",
    "        epoch_total += len(batch_labels)\n",
    "\n",
    "        if batch_number % log_every == log_every - 1:\n",
    "            if train:\n",
    "                print(\"[Batch {:5d}] Loss: {:.3f} Top-1 Error: {:.3f} Top-5 Error: {:.3f}\".format(\n",
    "                    batch_number + 1,\n",
    "                    running_loss / log_every,\n",
    "                    top_k_error(running_top_1, running_total),\n",
    "                    top_k_error(running_top_5, running_total)\n",
    "                ))\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_top_1 = 0.0\n",
    "            running_top_5 = 0.0\n",
    "            running_total = 0.0\n",
    "\n",
    "    return top_k_error(epoch_top_1, epoch_total), top_k_error(epoch_top_5, epoch_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1 # modify this to run different number of epochs\n",
    "\n",
    "for epoch_number in range(num_epochs):\n",
    "    train_top_1_error, train_top_5_error = run_epoch(train_loader, model, criterion, optimizer, train=True)\n",
    "    print(\"[Epoch {:3d}] Training Top-1 Error: {:.3f} Top-5 Error: {:.3f}\".format(\n",
    "        epoch_number + 1, train_top_1_error, train_top_5_error))\n",
    "\n",
    "    val_top_1_error, val_top_5_error = run_epoch(val_loader, model, criterion, train=False)\n",
    "    print(\"[Epoch {:3d}] Validation Top-1 Error: {:.3f} Top-5 Error: {:.3f}\".format(\n",
    "        epoch_number + 1, val_top_1_error, val_top_5_error))\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1200f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
